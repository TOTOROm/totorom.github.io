<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xie Mingjie(谢明洁)</title>
  
  
  <link href="http://www.seuxie.com/atom.xml" rel="self"/>
  
  <link href="http://www.seuxie.com/"/>
  <updated>2021-09-14T11:47:38.945Z</updated>
  <id>http://www.seuxie.com/</id>
  
  <author>
    <name>Xie Mingjie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>预训练模型</title>
    <link href="http://www.seuxie.com/2021/09/14/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://www.seuxie.com/2021/09/14/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-09-14T11:18:03.000Z</published>
    <updated>2021-09-14T11:47:38.945Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>预训练模型就意味着把人类的语言知识，先学了一个东西，然后再代入到某个具体任务，就顺手了，就是这么一个简单的道理。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概括&quot;&gt;&lt;a href=&quot;#概括&quot; class=&quot;headerlink&quot; title=&quot;概括&quot;&gt;&lt;/a&gt;概括&lt;/h2&gt;&lt;p&gt;预训练模型就意味着把人类的语言知识，先学了一个东西，然后再代入到某个具体任务，就顺手了，就是这么一个简单的道理。&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
</feed>
